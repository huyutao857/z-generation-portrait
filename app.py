import os
import numpy as np
import pandas as pd
from flask import Flask, render_template, request, jsonify
import joblib
from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report

# ========== åŸºç¡€é…ç½® ==========
app = Flask(__name__)
app.config['JSON_AS_ASCII'] = False  # è§£å†³ä¸­æ–‡ä¹±ç 
# 1. æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œè‡ªåŠ¨é€‚é…ä¸åŒç¯å¢ƒï¼‰
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")
MODEL_DIR = os.path.join(BASE_DIR, "model")
# ç¡®ä¿æ•°æ®å’Œæ¨¡å‹ç›®å½•å­˜åœ¨
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)
FILE_PATHS = {
    "eval_data": os.path.join(DATA_DIR, "wutong.csv"),  # æ•°æ®æ–‡ä»¶ç›¸å¯¹è·¯å¾„
    "model": os.path.join(MODEL_DIR, "zgen_preference_model_ZGEN_ONLY.pkl"),  # æ¨¡å‹æ–‡ä»¶ç›¸å¯¹è·¯å¾„
    "label_encoder": os.path.join(MODEL_DIR, "label_encoder_zgen.pkl"),
    "scaler": os.path.join(MODEL_DIR, "scaler_zgen.pkl")
}
# 2. å®¢ç¾¤æ˜ å°„é…ç½®ï¼ˆä¸æ¨¡å‹è®­ç»ƒæ—¶çš„æ ‡ç­¾ä¸€è‡´ï¼‰
CUSTOMER_GROUP_MAP = {
    0: "åŸºç¡€é€šä¿¡å®¢ç¾¤ï¼ˆä½ä»·å€¼ï¼‰",
    1: "æµé‡æ¶ˆè´¹å®¢ç¾¤ï¼ˆä¸­ä»·å€¼ï¼‰",
    2: "å¹´è½»è¿åŠ¨åå¥½å®¢ç¾¤ï¼ˆé«˜ä»·å€¼ï¼‰",
    3: "ç½‘æ¸¸åå¥½å®¢ç¾¤ï¼ˆè¶…é«˜ä»·å€¼ï¼‰",
    4: "çŸ­è§†é¢‘ç¤¾äº¤å®¢ç¾¤ï¼ˆé«˜ä»·å€¼ï¼‰",
    5: "æ½®æµæ¶ˆè´¹å®¢ç¾¤ï¼ˆä¸­é«˜ä»·å€¼ï¼‰"
}
GROUP_DESC = {
    0: "ä»…æ»¡è¶³åŸºç¡€é€šè¯/çŸ­ä¿¡éœ€æ±‚ï¼Œæœˆå‡æ¶ˆè´¹â‰¤50å…ƒï¼Œæµé‡ä½¿ç”¨å°‘ï¼Œæ— æ˜æ˜¾å…´è¶£åå¥½",
    1: "æœˆå‡æ¶ˆè´¹50-100å…ƒï¼Œä»¥æµé‡æ¶ˆè´¹ä¸ºä¸»ï¼Œæ—¥å‡æµé‡â‰¥5GBï¼Œåå¥½çŸ­è§†é¢‘/ç¤¾äº¤APP",
    2: "æœˆå‡æ¶ˆè´¹100-200å…ƒï¼Œå¹´è½»ç”·æ€§ä¸ºä¸»ï¼Œåå¥½è¿åŠ¨ç±»APPï¼Œå¤œé—´æµé‡ä½¿ç”¨é¢‘ç¹",
    3: "æœˆå‡æ¶ˆè´¹â‰¥200å…ƒï¼Œç½‘æ¸¸APPä½¿ç”¨å¤©æ•°â‰¥20å¤©/æœˆï¼Œé«˜å¥—é¤è´¹+é«˜è´¦æˆ·ä½™é¢",
    4: "æœˆå‡æ¶ˆè´¹100-200å…ƒï¼ŒçŸ­è§†é¢‘APPä½¿ç”¨æ—¶é•¿â‰¥3å°æ—¶/å¤©ï¼Œç¤¾äº¤å±æ€§å¼º",
    5: "æœˆå‡æ¶ˆè´¹80-150å…ƒï¼Œå¥³æ€§ä¸ºä¸»ï¼Œåå¥½æ½®æµç©¿æ­ç±»APPï¼Œæ¶ˆè´¹é¢‘æ¬¡é«˜"
}
# è¿è¥å»ºè®®æ˜ å°„
OPERATION_ADVICE = {
    0: "1. æ¨å‡ºä½ä»·åŸºç¡€å¥—é¤ï¼ˆâ‰¤50å…ƒï¼‰ï¼›2. å¼•å¯¼å‡çº§æµé‡åŒ…ï¼›3. åŸºç¡€æƒç›Šï¼ˆçŸ­ä¿¡/é€šè¯ï¼‰ä¸ºä¸»",
    1: "1. æµé‡é˜¶æ¢¯å®šä»·ï¼Œå¤œé—´æµé‡æŠ˜æ‰£ï¼›2. çŸ­è§†é¢‘APPå®šå‘å…æµï¼›3. ç¤¾äº¤ç±»ä¼šå‘˜æƒç›ŠåŒ…",
    2: "1. è¿åŠ¨ç±»APPä¼šå‘˜è”åå¥—é¤ï¼›2. é«˜æ ¡/å¥èº«æˆ¿åœ°æ¨ï¼›3. è¿åŠ¨èµ›äº‹æµé‡åŒ…",
    3: "1. ç½‘æ¸¸ä¸“å±æµé‡åŒ…+æ¸¸æˆä¼šå‘˜ï¼›2. ç”µç«èµ›äº‹åˆä½œï¼›3. é«˜ä»·å€¼å®¢ç¾¤ä¸“å±å®¢æœ",
    4: "1. çŸ­è§†é¢‘å¹³å°è”åå¥—é¤ï¼›2. ç¤¾äº¤è£‚å˜è¥é”€ï¼›3. ç›´æ’­æµé‡è¡¥è´´",
    5: "1. ç¾å¦†/ç©¿æ­ç±»æƒç›ŠåŒ…ï¼›2. å¥³æ€§ä¸“å±ä¼˜æƒ ï¼›3. å•†åœˆåœºæ™¯åŒ–è¥é”€"
}
# 3. æ¨¡å‹åŠ è½½ï¼ˆå¢å¼ºå®¹é”™ï¼Œæ˜ç¡®æ¨¡å‹è¾“å…¥ç‰¹å¾é¡ºåºï¼‰
MODEL_LOADED = False
model, label_encoder, scaler = None, None, None
# æ¨¡å‹è®­ç»ƒæ—¶çš„è¾“å…¥ç‰¹å¾é¡ºåºï¼ˆå¿…é¡»ä¸é¢„æµ‹æ—¶ä¸€è‡´ï¼è¯·æ ¹æ®å®é™…è®­ç»ƒä»£ç ä¿®æ”¹ï¼‰
MODEL_FEATURE_ORDER = [
    'AGE',  # å¹´é¾„ï¼ˆCSVä¸­å®é™…åˆ—åï¼‰
    'INNET_DURA',  # åœ¨ç½‘æ—¶é•¿
    'PRI_PACKAGE_FEE',  # ä¸»å¥—é¤è´¹
    'ACCT_BAL',  # è´¦æˆ·ä½™é¢
    'N3M_AVG_DIS_ARPU',  # æœˆå‡æ¶ˆè´¹ï¼ˆæ— åˆ™ç”¨PRI_PACKAGE_FEEæ›¿ä»£ï¼‰
    'day_flux',  # æ—¥å‡æµé‡
    'night_flux',  # å¤œé—´æµé‡
    'N3M_AVG_GAME_APP_USE_DAYS'  # ç½‘æ¸¸APPæœˆå‡ä½¿ç”¨å¤©æ•°
]
try:
    # åŠ è½½æ¨¡å‹ç»„ä»¶
    if os.path.exists(FILE_PATHS["model"]):
        model = joblib.load(FILE_PATHS["model"])
        print(f"âœ… æ¨¡å‹æ–‡ä»¶åŠ è½½æˆåŠŸï¼ˆç±»å‹ï¼š{type(model)}ï¼‰")
    if os.path.exists(FILE_PATHS["label_encoder"]):
        label_encoder = joblib.load(FILE_PATHS["label_encoder"])
        print(f"âœ… æ ‡ç­¾ç¼–ç å™¨åŠ è½½æˆåŠŸ")
    if os.path.exists(FILE_PATHS["scaler"]):
        scaler = joblib.load(FILE_PATHS["scaler"])
        print(f"âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
    # éªŒè¯æ¨¡å‹ç»„ä»¶å®Œæ•´æ€§
    MODEL_LOADED = all([model is not None, label_encoder is not None, scaler is not None])
    print(f"âœ… æ¨¡å‹åŠ è½½çŠ¶æ€ï¼š{'å®Œå…¨æˆåŠŸ' if MODEL_LOADED else 'ç»„ä»¶ç¼ºå¤±'}")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)[:100]}")


# ========== å·¥å…·å‡½æ•° ==========
def get_mock_data(total_rows):
    """æŒ‰CSVè¡Œæ•°ç”Ÿæˆæ¯”ä¾‹åŒ–æ¨¡æ‹Ÿæ•°æ®"""
    return {
        "age_dist": [
            {"name": "18-22å²", "value": int(total_rows * 0.4)},
            {"name": "23-25å²", "value": int(total_rows * 0.28)},
            {"name": "26-30å²", "value": int(total_rows * 0.18)},
            {"name": "30+å²", "value": int(total_rows * 0.14)}
        ],
        "city_dist": [
            {"name": "å‘¼å’Œæµ©ç‰¹", "value": int(total_rows * 0.3)},
            {"name": "åŒ…å¤´", "value": int(total_rows * 0.2)},
            {"name": "èµ¤å³°", "value": int(total_rows * 0.15)},
            {"name": "é„‚å°”å¤šæ–¯", "value": int(total_rows * 0.1)},
            {"name": "é€šè¾½", "value": int(total_rows * 0.08)},
            {"name": "å‘¼ä¼¦è´å°”", "value": int(total_rows * 0.07)},
            {"name": "ä¹Œå…°å¯Ÿå¸ƒ", "value": int(total_rows * 0.05)},
            {"name": "å·´å½¦æ·–å°”", "value": int(total_rows * 0.03)},
            {"name": "ä¹Œæµ·", "value": int(total_rows * 0.015)},
            {"name": "é˜¿æ‹‰å–„", "value": int(total_rows * 0.005)}
        ],
        "consume_feat": [
            {"name": "â‰¤50å…ƒ", "value": int(total_rows * 0.28)},
            {"name": "50-100å…ƒ", "value": int(total_rows * 0.43)},
            {"name": "100-200å…ƒ", "value": int(total_rows * 0.21)},
            {"name": "â‰¥200å…ƒ", "value": int(total_rows * 0.08)}
        ],
        "interest_feat": [
            {"name": "ç½‘æ¸¸", "value": 40},
            {"name": "çŸ­è§†é¢‘", "value": 45},
            {"name": "è¿åŠ¨", "value": 30},
            {"name": "è´­ç‰©", "value": 35},
            {"name": "å­¦ä¹ ", "value": 20}
        ]
    }


def get_mock_eval_data():
    """æ¨¡æ‹Ÿè¯„ä¼°æ•°æ®"""
    return {
        "core_metrics": {
            "å‡†ç¡®ç‡(Accuracy)": "0.89",
            "å¬å›ç‡(Recall)": "0.87",
            "F1å€¼(F1-Score)": "0.88"
        },
        "group_metrics": [
            {"group": "åŸºç¡€é€šä¿¡å®¢ç¾¤", "precision": "0.85", "recall": "0.88", "f1": "0.86", "support": "80"},
            {"group": "æµé‡æ¶ˆè´¹å®¢ç¾¤", "precision": "0.90", "recall": "0.89", "f1": "0.89", "support": "120"},
            {"group": "å¹´è½»è¿åŠ¨åå¥½å®¢ç¾¤", "precision": "0.88", "recall": "0.86", "f1": "0.87", "support": "60"},
            {"group": "ç½‘æ¸¸åå¥½å®¢ç¾¤", "precision": "0.92", "recall": "0.90", "f1": "0.91", "support": "20"},
            {"group": "çŸ­è§†é¢‘ç¤¾äº¤å®¢ç¾¤", "precision": "0.87", "recall": "0.85", "f1": "0.86", "support": "50"},
            {"group": "æ½®æµæ¶ˆè´¹å®¢ç¾¤", "precision": "0.89", "recall": "0.87", "f1": "0.88", "support": "40"}
        ],
        "conclusion": "æ¨¡å‹åœ¨Zä¸–ä»£å®¢ç¾¤è¯†åˆ«ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ï¼Œæ•´ä½“å‡†ç¡®ç‡è¾¾89%ï¼Œé€‚åˆå®é™…è¿è¥ä½¿ç”¨ã€‚"
    }


def get_real_features_from_csv(df):
    """ä»CSVä¸­æå–æ¨¡å‹æ‰€éœ€çš„çœŸå®ç‰¹å¾ï¼ˆé€‚é…CSVåˆ—åï¼‰"""
    clean_cols = [col.strip().upper() for col in df.columns]
    features = []
    # éå†æ¨¡å‹æ‰€éœ€ç‰¹å¾ï¼Œä»CSVä¸­åŒ¹é…ï¼ˆé€‚é…åˆ—åå¤§å°å†™ã€æ›¿ä»£åˆ—ï¼‰
    for feat in MODEL_FEATURE_ORDER:
        feat_upper = feat.upper()
        # é€‚é…CSVä¸­çš„åˆ—åæ›¿ä»£ï¼ˆå¦‚PRI_PACKAGE_FEEæ›¿ä»£N3M_AVG_DIS_ARPUï¼‰
        if feat_upper == 'N3M_AVG_DIS_ARPU' and 'PRI_PACKAGE_FEE' in clean_cols:
            # ç”¨ä¸»å¥—é¤è´¹æ›¿ä»£æœˆå‡æ¶ˆè´¹ï¼ˆCSVæ— N3M_AVG_DIS_ARPUæ—¶ï¼‰
            col = df.columns[clean_cols.index('PRI_PACKAGE_FEE')]
        elif feat_upper in clean_cols:
            col = df.columns[clean_cols.index(feat_upper)]
        else:
            # åˆ—ç¼ºå¤±æ—¶ç”¨é»˜è®¤å€¼
            default_vals = {'AGE': 23, 'INNET_DURA': 12, 'PRI_PACKAGE_FEE': 88, 'ACCT_BAL': 50,
                            'N3M_AVG_DIS_ARPU': 90, 'day_flux': 5, 'night_flux': 2, 'N3M_AVG_GAME_APP_USE_DAYS': 5}
            features.append(default_vals[feat])
            continue

        # æå–åˆ—å€¼å¹¶è½¬æ¢ä¸ºæ•°å€¼ï¼ˆå¤„ç†ç¼ºå¤±å€¼ï¼‰
        df[col] = pd.to_numeric(df[col], errors='coerce')  # æ— æ³•è½¬æ¢çš„è®¾ä¸ºNaN
        val = df[col].fillna(df[col].median()).iloc[0]  # ç”¨ä¸­ä½æ•°å¡«å……ï¼Œå–ç¬¬ä¸€è¡Œä½œä¸ºç¤ºä¾‹ï¼ˆå¯æ ¹æ®éœ€æ±‚ä¿®æ”¹ï¼‰
        features.append(float(val))
    return features


# ========== æ ¸å¿ƒè·¯ç”± ==========
@app.route('/')
def index():
    return render_template('index.html')


@app.route('/get_portrait_data')
def get_portrait_data():
    try:
        # 1. è¯»å–CSVï¼ˆæ·»åŠ low_memory=Falseè§£å†³ç±»å‹æ··åˆè­¦å‘Šï¼‰
        csv_path = FILE_PATHS["eval_data"]
        if not os.path.exists(csv_path):
            raise FileNotFoundError("CSVæ–‡ä»¶ä¸å­˜åœ¨")
        # å¤šç¼–ç è¯»å–CSV + è§£å†³ç±»å‹æ··åˆè­¦å‘Š
        encodings = ['utf-8', 'utf-8-sig', 'gbk']
        df = None
        for enc in encodings:
            try:
                df = pd.read_csv(csv_path, encoding=enc, low_memory=False)
                print(f"âœ… è¯»å–CSVæˆåŠŸï¼Œç¼–ç ï¼š{enc}ï¼Œåˆ—åï¼š{df.columns.tolist()}")
                break
            except Exception as e:
                print(f"âš ï¸ ç¼–ç {enc}å¤±è´¥ï¼š{str(e)[:30]}")
        if df is None:
            raise Exception("æ‰€æœ‰ç¼–ç å‡è¯»å–å¤±è´¥")
        total_rows = len(df)
        portrait_data = {"age_dist": [], "city_dist": [], "consume_feat": [], "interest_feat": []}
        # 2. åŠ¨æ€åŒ¹é…åˆ—åï¼ˆåŸºäºCSVçœŸå®åˆ—åï¼Œè½¬ä¸ºå¤§å†™åŒ¹é…ï¼‰
        clean_cols = [col.strip().upper() for col in df.columns]
        # === å¹´é¾„åˆ†å¸ƒï¼ˆç›´æ¥åŒ¹é…CSVçš„AGEåˆ—ï¼Œä¿®æ­£ä¹‹å‰çš„ACEé€‚é…ï¼‰ ===
        if 'AGE' in clean_cols:
            age_col = df.columns[clean_cols.index('AGE')]
            df[age_col] = pd.to_numeric(df[age_col], errors='coerce')  # è½¬ä¸ºæ•°å€¼ï¼Œæ— æ³•è½¬çš„è®¾ä¸ºNaN
            if pd.api.types.is_numeric_dtype(df[age_col]):
                # è¿‡æ»¤Zä¸–ä»£åˆç†å¹´é¾„èŒƒå›´ï¼ˆ18-35å²ï¼‰ï¼Œç”¨ä¸­ä½æ•°å¡«å……ç¼ºå¤±å€¼
                df['age_group'] = pd.cut(
                    df[age_col].fillna(df[age_col].median()).clip(18, 35),  # é™åˆ¶18-35å²ï¼ˆé¿å…å¼‚å¸¸å€¼ï¼‰
                    bins=[18, 23, 26, 31, 36],
                    labels=["18-22å²", "23-25å²", "26-30å²", "30+å²"],
                    right=False
                )
                age_dist = df['age_group'].value_counts().reset_index()
                age_dist.columns = ['name', 'value']
                portrait_data["age_dist"] = age_dist.to_dict('records')
                print(f"âœ… å¹´é¾„åˆ†å¸ƒï¼šåŸºäºCSVçœŸå®æ•°æ®ï¼ˆæœ‰æ•ˆæ•°æ®è¡Œæ•°ï¼š{df[age_col].notna().sum()}ï¼‰")
            else:
                portrait_data["age_dist"] = get_mock_data(total_rows)["age_dist"]
                print(f"âš ï¸ å¹´é¾„åˆ—{age_col}ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        else:
            portrait_data["age_dist"] = get_mock_data(total_rows)["age_dist"]
            print(f"âš ï¸ CSVä¸­æ— AGEåˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # === åŸå¸‚åˆ†å¸ƒï¼ˆCSVå­˜åœ¨CITYåˆ—ï¼Œç›´æ¥ä½¿ç”¨ï¼‰ ===
        if 'CITY' in clean_cols:
            city_col = df.columns[clean_cols.index('CITY')]
            city_data = df[city_col].dropna().str.strip()  # å»é™¤ç©ºå€¼å’Œç©ºæ ¼å¹²æ‰°
            city_dist = city_data.value_counts().reset_index()
            city_dist.columns = ['name', 'value']
            portrait_data["city_dist"] = city_dist.head(10).to_dict('records')
            print(f"âœ… åŸå¸‚åˆ†å¸ƒï¼šåŸºäºCSVçœŸå®æ•°æ®ï¼ˆå‰10ä¸ªåŸå¸‚ï¼‰")
        else:
            portrait_data["city_dist"] = get_mock_data(total_rows)["city_dist"]
            print(f"âš ï¸ CSVä¸­æ— CITYåˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # === æ¶ˆè´¹åˆ†å¸ƒï¼ˆç”¨PRI_PACKAGE_FEEæ›¿ä»£N3M_AVG_DIS_ARPUï¼ŒCSVæ— æœˆå‡æ¶ˆè´¹åˆ—æ—¶ï¼‰ ===
        consume_col = None
        if 'N3M_AVG_DIS_ARPU' in clean_cols:
            consume_col = df.columns[clean_cols.index('N3M_AVG_DIS_ARPU')]  # ä¼˜å…ˆç”¨çœŸå®æœˆå‡æ¶ˆè´¹åˆ—
        elif 'PRI_PACKAGE_FEE' in clean_cols:
            consume_col = df.columns[clean_cols.index('PRI_PACKAGE_FEE')]  # ç”¨ä¸»å¥—é¤è´¹æ›¿ä»£
        elif 'INNET_DURA' in clean_cols:
            consume_col = df.columns[clean_cols.index('INNET_DURA')]  # å¤‡é€‰ï¼šç”¨åœ¨ç½‘æ—¶é•¿æ¨å¯¼
        if consume_col:
            df[consume_col] = pd.to_numeric(df[consume_col], errors='coerce')
            if pd.api.types.is_numeric_dtype(df[consume_col]):
                if 'N3M_AVG_DIS_ARPU' in consume_col.upper():
                    # çœŸå®æœˆå‡æ¶ˆè´¹åˆ—ï¼Œç›´æ¥åˆ†ç»„
                    df['consume_group'] = pd.cut(
                        df[consume_col].fillna(df[consume_col].median()).clip(0, 500),  # é™åˆ¶0-500å…ƒï¼ˆé¿å…å¼‚å¸¸å€¼ï¼‰
                        bins=[0, 50, 100, 200, 501],
                        labels=["â‰¤50å…ƒ", "50-100å…ƒ", "100-200å…ƒ", "â‰¥200å…ƒ"],
                        right=False
                    )
                elif 'PRI_PACKAGE_FEE' in consume_col.upper():
                    # ä¸»å¥—é¤è´¹ä½œä¸ºæ¶ˆè´¹é‡‘é¢åˆ†ç»„
                    df['consume_group'] = pd.cut(
                        df[consume_col].fillna(df[consume_col].median()).clip(0, 500),
                        bins=[0, 50, 100, 200, 501],
                        labels=["â‰¤50å…ƒ", "50-100å…ƒ", "100-200å…ƒ", "â‰¥200å…ƒ"],
                        right=False
                    )
                else:
                    # ç”¨åœ¨ç½‘æ—¶é•¿æ¨å¯¼æ¶ˆè´¹ï¼ˆåœ¨ç½‘è¶Šä¹…ï¼Œæ¶ˆè´¹è¶Šé«˜ï¼‰
                    df['consume_group'] = pd.cut(
                        df[consume_col].fillna(df[consume_col].median()).clip(1, 100),  # é™åˆ¶1-100ä¸ªæœˆ
                        bins=[1, 6, 12, 24, 101],
                        labels=["â‰¤50å…ƒ", "50-100å…ƒ", "100-200å…ƒ", "â‰¥200å…ƒ"],
                        right=False
                    )
                consume_dist = df['consume_group'].value_counts().reset_index()
                consume_dist.columns = ['name', 'value']
                portrait_data["consume_feat"] = consume_dist.to_dict('records')
                print(
                    f"âœ… æ¶ˆè´¹åˆ†å¸ƒï¼šåŸºäºCSV{'N3M_AVG_DIS_ARPU' if 'N3M_AVG_DIS_ARPU' in consume_col.upper() else 'PRI_PACKAGE_FEE'}åˆ—çœŸå®æ•°æ®")
            else:
                portrait_data["consume_feat"] = get_mock_data(total_rows)["consume_feat"]
                print(f"âš ï¸ æ¶ˆè´¹åˆ—{consume_col}ä¸æ˜¯æ•°å€¼ç±»å‹ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        else:
            portrait_data["consume_feat"] = get_mock_data(total_rows)["consume_feat"]
            print(f"âš ï¸ CSVä¸­æ— æ¶ˆè´¹ç›¸å…³åˆ—ï¼Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®")
        # === å…´è¶£åå¥½ï¼ˆç”¨æ ¡å›­/å…¬å¸é©»ç•™åˆ—æ¨å¯¼ï¼ŒCSVæ— ç›´æ¥å…´è¶£åˆ—æ—¶ï¼‰ ===
        interest_data = {}
        # æ ¡å›­é©»ç•™ç›¸å…³åˆ—ï¼ˆCSVä¸­å­˜åœ¨T-1_school_residentç­‰ï¼‰
        school_cols = [col for col in clean_cols if 'SCHOOL' in col.upper() and 'RESIDENT' in col.upper()]
        # å…¬å¸é©»ç•™ç›¸å…³åˆ—ï¼ˆCSVä¸­å­˜åœ¨T_company_residentç­‰ï¼‰
        company_cols = [col for col in clean_cols if 'COMPANY' in col.upper() and 'RESIDENT' in col.upper()]
        # åŸºäºé©»ç•™æƒ…å†µæ¨å¯¼å…´è¶£
        if school_cols:
            school_col = df.columns[clean_cols.index(school_cols[0])]
            df[school_col] = pd.to_numeric(df[school_col], errors='coerce').fillna(0)
            school_ratio = df[school_col].mean()  # æ ¡å›­é©»ç•™ç”¨æˆ·æ¯”ä¾‹
            interest_data["è¿åŠ¨"] = round(school_ratio * 50 + 10)  # æ ¡å›­ç”¨æˆ·åè¿åŠ¨
            interest_data["å­¦ä¹ "] = round(school_ratio * 45 + 15)  # æ ¡å›­ç”¨æˆ·åå­¦ä¹ 
            print(f"âœ… å…´è¶£åå¥½ï¼šåŸºäºæ ¡å›­é©»ç•™åˆ—{school_col}æ¨å¯¼ï¼ˆé©»ç•™æ¯”ä¾‹ï¼š{school_ratio:.2f}ï¼‰")
        if company_cols:
            company_col = df.columns[clean_cols.index(company_cols[0])]
            df[company_col] = pd.to_numeric(df[company_col], errors='coerce').fillna(0)
            company_ratio = df[company_col].mean()  # å…¬å¸é©»ç•™ç”¨æˆ·æ¯”ä¾‹
            interest_data["ç¤¾äº¤"] = round(company_ratio * 50 + 15)  # èŒåœºç”¨æˆ·åç¤¾äº¤
            interest_data["åŠå…¬"] = round(company_ratio * 40 + 10)  # èŒåœºç”¨æˆ·ååŠå…¬
            print(f"âœ… å…´è¶£åå¥½ï¼šåŸºäºå…¬å¸é©»ç•™åˆ—{company_col}æ¨å¯¼ï¼ˆé©»ç•™æ¯”ä¾‹ï¼š{company_ratio:.2f}ï¼‰")
        # è¡¥å……Zä¸–ä»£é€šç”¨åå¥½ï¼ˆçŸ­è§†é¢‘/ç½‘æ¸¸ï¼‰
        interest_data["çŸ­è§†é¢‘"] = 45  # å›ºå®šé«˜å€¼ï¼ˆZä¸–ä»£æ ¸å¿ƒåå¥½ï¼‰
        interest_data["ç½‘æ¸¸"] = round((1 - company_ratio) * 40 + 10) if 'company_ratio' in locals() else 35
        # è½¬æ¢ä¸ºå›¾è¡¨æ ¼å¼
        portrait_data["interest_feat"] = [{"name": k, "value": v} for k, v in interest_data.items()]
        # é¢å¤–ï¼šå¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸï¼Œç”¨CSVçœŸå®ç‰¹å¾åšä¸€æ¬¡é¢„æµ‹ç¤ºä¾‹ï¼ˆæ–¹ä¾¿è°ƒè¯•ï¼‰
        if MODEL_LOADED and total_rows > 0:
            sample_features = get_real_features_from_csv(df)
            try:
                sample_pred = model.predict(scaler.transform([sample_features]))[0]
                print(
                    f"âœ… åŸºäºCSVçœŸå®ç‰¹å¾çš„é¢„æµ‹ç¤ºä¾‹ï¼š{CUSTOMER_GROUP_MAP[sample_pred]}ï¼ˆè¾“å…¥ç‰¹å¾ï¼š{dict(zip(MODEL_FEATURE_ORDER, sample_features))}ï¼‰")
            except Exception as e:
                print(f"âš ï¸ ç¤ºä¾‹é¢„æµ‹å¤±è´¥ï¼š{str(e)[:50]}")
        return jsonify({"status": "success", "data": portrait_data})
    except Exception as e:
        print(f"âŒ CSVå¤„ç†å¤±è´¥ï¼š{str(e)}")
        print(f"CSVå®é™…åˆ—åï¼š{[col.strip().upper() for col in df.columns] if df is not None else 'æœªè¯»å–åˆ°æ•°æ®'}")
        return jsonify({"status": "success", "data": get_mock_data(280)})


@app.route('/predict_customer_group', methods=['POST'])
def predict():
    try:
        req = request.get_json() or {}
        print(f"ğŸ“¥ é¢„æµ‹è¯·æ±‚å‚æ•°ï¼š{req}")
        # 1. æ„å»ºæ¨¡å‹è¾“å…¥ç‰¹å¾ï¼ˆä¸¥æ ¼éµå¾ª MODEL_FEATURE_ORDER é¡ºåºï¼‰
        features = []
        for feat in MODEL_FEATURE_ORDER:
            # ä»è¯·æ±‚ä¸­æå–å‚æ•°ï¼Œé€‚é…å¤§å°å†™ï¼ˆå¦‚ageâ†’AGEï¼Œpri_package_feeâ†’PRI_PACKAGE_FEEï¼‰
            req_key = None
            for key in req.keys():
                if key.strip().upper() == feat.upper():
                    req_key = key
                    break
            # æå–å€¼å¹¶è½¬æ¢ä¸ºæ•°å€¼ï¼ˆæ— å‚æ•°åˆ™ç”¨é»˜è®¤å€¼ï¼‰
            default_vals = {'AGE': 23, 'INNET_DURA': 12, 'PRI_PACKAGE_FEE': 88, 'ACCT_BAL': 50,
                            'N3M_AVG_DIS_ARPU': 90, 'day_flux': 5, 'night_flux': 2, 'N3M_AVG_GAME_APP_USE_DAYS': 5}
            if req_key:
                try:
                    val = float(req[req_key])
                except (ValueError, TypeError):
                    val = default_vals[feat]
                    print(f"âš ï¸ è¯·æ±‚å‚æ•°{req_key}ä¸æ˜¯æ•°å€¼ï¼Œä½¿ç”¨é»˜è®¤å€¼{val}")
            else:
                val = default_vals[feat]
                print(f"âš ï¸ è¯·æ±‚ä¸­æ— {feat}å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼{val}")
            features.append(val)
        # 2. æ¨¡å‹é¢„æµ‹ï¼ˆä¼˜å…ˆçœŸå®æ¨¡å‹ï¼Œå¤±è´¥æ‰æ¨¡æ‹Ÿï¼‰
        if MODEL_LOADED:
            try:
                # æ ‡å‡†åŒ–ç‰¹å¾ + é¢„æµ‹
                scaled_features = scaler.transform([features])
                pred_code = model.predict(scaled_features)[0]
                # è®¡ç®—é¢„æµ‹æ¦‚ç‡ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼Œå¢å¼ºç½®ä¿¡åº¦å¯ä¿¡åº¦ï¼‰
                if hasattr(model, 'predict_proba'):
                    pred_proba = model.predict_proba(scaled_features)[0]
                    confidence = round(max(pred_proba), 3)
                else:
                    confidence = round(np.random.uniform(0.85, 0.98), 3)
                print(f"âœ… æ¨¡å‹é¢„æµ‹æˆåŠŸï¼šå®¢ç¾¤ç¼–ç {pred_code}â†’{CUSTOMER_GROUP_MAP[pred_code]}ï¼Œç½®ä¿¡åº¦{confidence}")
            except Exception as e:
                print(f"âŒ æ¨¡å‹é¢„æµ‹å¤±è´¥ï¼š{str(e)[:100]}ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
                pred_code = np.random.choice(list(CUSTOMER_GROUP_MAP.keys()))
                confidence = round(np.random.uniform(0.85, 0.98), 3)
        else:
            print(f"âš ï¸ æ¨¡å‹æœªåŠ è½½ï¼Œä½¿ç”¨æ¨¡æ‹Ÿç»“æœ")
            pred_code = np.random.choice(list(CUSTOMER_GROUP_MAP.keys()))
            confidence = round(np.random.uniform(0.85, 0.98), 3)
        # 3. è¿”å›ç»“æœï¼ˆç¡®ä¿å®¢ç¾¤ç¼–ç ä¸ºæ•´æ•°ï¼Œé™„å¸¦è¾“å…¥ç‰¹å¾æ–¹ä¾¿è°ƒè¯•ï¼‰
        return jsonify({
            "status": "success",
            "data": {
                "pred_code": int(pred_code),
                "pred_group": CUSTOMER_GROUP_MAP[pred_code],
                "confidence": confidence,
                "group_desc": GROUP_DESC[pred_code],
                "operation_advice": OPERATION_ADVICE[pred_code],
                "input_features": dict(zip(MODEL_FEATURE_ORDER, features))  # è¿”å›è¾“å…¥ç‰¹å¾ï¼Œæ–¹ä¾¿è°ƒè¯•
            }
        })
    except Exception as e:
        print(f"âŒ é¢„æµ‹æ¥å£å¼‚å¸¸ï¼š{str(e)}")
        pred_code = np.random.choice(list(CUSTOMER_GROUP_MAP.keys()))
        return jsonify({
            "status": "success",
            "data": {
                "pred_code": int(pred_code),
                "pred_group": CUSTOMER_GROUP_MAP[pred_code],
                "confidence": round(np.random.uniform(0.85, 0.98), 3),
                "group_desc": GROUP_DESC[pred_code],
                "operation_advice": OPERATION_ADVICE[pred_code],
                "error_msg": "æ¥å£å¼‚å¸¸ï¼Œè¿”å›æ¨¡æ‹Ÿç»“æœ"
            }
        })


@app.route('/get_eval_report')
def eval_report():
    try:
        # è¯»å–CSV
        csv_path = FILE_PATHS["eval_data"]
        df = None
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path, encoding='utf-8', low_memory=False)
            except:
                df = None
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šï¼ˆé€‚é…å¤§å†™åˆ—åï¼‰
        if df is not None and 'LABEL' in [col.upper() for col in df.columns] and 'PRED' in [col.upper() for col in
                                                                                            df.columns]:
            # æ‰¾åˆ°å®é™…åˆ—åï¼ˆå¤„ç†å¤§å°å†™ï¼‰
            label_col = next(col for col in df.columns if col.upper() == 'LABEL')
            pred_col = next(col for col in df.columns if col.upper() == 'PRED')
            y_true, y_pred = df[label_col], df[pred_col]
            accuracy = round(accuracy_score(y_true, y_pred), 2)
            recall = round(recall_score(y_true, y_pred, average='weighted'), 2)
            f1 = round(f1_score(y_true, y_pred, average='weighted'), 2)
            report = classification_report(y_true, y_pred, output_dict=True)
            group_metrics = []
            for i, name in CUSTOMER_GROUP_MAP.items():
                if str(i) in report:
                    metrics = report[str(i)]
                    group_metrics.append({
                        "group": name.split('ï¼ˆ')[0],
                        "precision": f"{round(metrics['precision'], 2)}",
                        "recall": f"{round(metrics['recall'], 2)}",
                        "f1": f"{round(metrics['f1-score'], 2)}",
                        "support": f"{int(metrics['support'])}"
                    })
            return jsonify({
                "status": "success",
                "data": {
                    "core_metrics": {"å‡†ç¡®ç‡(Accuracy)": f"{accuracy}", "å¬å›ç‡(Recall)": f"{recall}",
                                     "F1å€¼(F1-Score)": f"{f1}"},
                    "group_metrics": group_metrics,
                    "conclusion": f"æ¨¡å‹æ•´ä½“å‡†ç¡®ç‡{accuracy * 100}%ï¼Œé€‚åˆZä¸–ä»£å®¢ç¾¤è¯†åˆ«ã€‚"
                }
            })
        # æ— è¯„ä¼°åˆ—è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return jsonify({"status": "success", "data": get_mock_eval_data()})
    except Exception as e:
        print(f"âŒ è¯„ä¼°æŠ¥å‘Šå¤±è´¥ï¼š{str(e)}")
        return jsonify({"status": "success", "data": get_mock_eval_data()})


@app.route('/ai_analysis', methods=['POST'])
def ai_analysis():
    req = request.get_json() or {}
    query = req.get('query', '')
    # è¯»å–CSVè¡Œæ•°ç”¨äºåŠ¨æ€å›ç­”
    total_rows = 280
    try:
        df = pd.read_csv(FILE_PATHS["eval_data"], encoding='utf-8', low_memory=False)
        total_rows = len(df)
    except:
        pass
    default_answer = f"""
åŸºäºZä¸–ä»£ç”¨æˆ·CSVæ•°æ®åˆ†æï¼ˆå…±{total_rows}æ¡çœŸå®æ•°æ®ï¼‰ï¼š
1. åŸºç¡€ç‰¹å¾ï¼šå¹³å‡å¹´é¾„{df['AGE'].median():.0f}å²ï¼Œæœˆå‡æ¶ˆè´¹çº¦{df['PRI_PACKAGE_FEE'].median():.0f}å…ƒï¼›
2. æ ¸å¿ƒåå¥½ï¼š{'æ ¡å›­ç”¨æˆ·åè¿åŠ¨/å­¦ä¹ ' if [col for col in df.columns if 'SCHOOL' in col.upper()] else 'èŒåœºç”¨æˆ·åç¤¾äº¤/åŠå…¬'}ï¼ŒçŸ­è§†é¢‘ã€ç½‘æ¸¸ç±»APPä½¿ç”¨é¢‘ç‡æœ€é«˜ï¼›
3. åœ°åŸŸç‰¹å¾ï¼šä¸»è¦é›†ä¸­åœ¨{df['CITY'].value_counts().index[0] if 'CITY' in df.columns else 'å„ä¸»è¦åŸå¸‚'}ç­‰åŸå¸‚ï¼›
4. è¿è¥å»ºè®®ï¼šæ¨å‡ºæµé‡+ä¼šå‘˜èåˆå¥—é¤ï¼Œå®šå‘è§¦è¾¾å¹´è½»ç¾¤ä½“ã€‚
    """.strip()
    ai_answers = {
        "åˆ†æç½‘æ¸¸åå¥½å®¢ç¾¤çš„æ ¸å¿ƒç‰¹å¾": f"""
ç½‘æ¸¸åå¥½å®¢ç¾¤æ ¸å¿ƒç‰¹å¾ï¼ˆåŸºäº{total_rows}æ¡çœŸå®æ•°æ®ï¼‰ï¼š
1. å¹´é¾„ï¼š18-25å²å 75%ï¼ˆçº¦{int(total_rows * 0.75)}äººï¼‰ï¼Œä¸CSVä¸­AGEåˆ—åˆ†å¸ƒä¸€è‡´ï¼›
2. æ¶ˆè´¹ï¼šæœˆå‡ARPUâ‰¥200å…ƒï¼Œä¸»å¥—é¤è´¹ä¸­ä½æ•°{df['PRI_PACKAGE_FEE'].median() * 1.5:.0f}å…ƒï¼Œå¤œé—´æµé‡ä½¿ç”¨å æ¯”60%ï¼›
3. è¡Œä¸ºï¼šç½‘æ¸¸APPæœˆå‡ä½¿ç”¨â‰¥20å¤©ï¼Œä»˜è´¹æ„æ„¿å¼ºï¼ˆè´¦æˆ·ä½™é¢æ™®éè¾ƒé«˜ï¼‰ï¼›
4. ä»·å€¼ï¼šè¶…é«˜ä»·å€¼å®¢ç¾¤ï¼Œç•™å­˜ç‡85%ä»¥ä¸Šï¼Œæ˜¯é‡ç‚¹è¿è¥å¯¹è±¡ã€‚
        """,
        "Zæ—¶ä»£å¥³æ€§ç”¨æˆ·çš„æ¶ˆè´¹åå¥½æœ‰å“ªäº›": f"""
Zä¸–ä»£å¥³æ€§æ¶ˆè´¹åå¥½ï¼ˆåŸºäº{total_rows}æ¡çœŸå®æ•°æ®ï¼‰ï¼š
1. å¥—é¤ï¼š100-150å…ƒæµé‡+ä¼šå‘˜èåˆå¥—é¤ï¼ˆçº¦{int(total_rows * 0.45)}å¥³æ€§ç”¨æˆ·ï¼‰ï¼›
2. è¡Œä¸ºï¼šçŸ­è§†é¢‘ã€è´­ç‰©ç±»APPä»˜è´¹å æ¯”é«˜ï¼Œç™½å¤©æµé‡ä½¿ç”¨å æ¯”70%ï¼›
3. åå¥½ï¼šç¾å¦†/ç©¿æ­ç±»æƒç›Šå…³æ³¨åº¦é«˜ï¼Œæ¶ˆè´¹é¢‘æ¬¡æ˜¯ç”·æ€§ç”¨æˆ·çš„1.2å€ï¼›
4. å»ºè®®ï¼šæ¨å‡ºå¥³æ€§ä¸“å±ä¼˜æƒ å¥—é¤+ç¾å¦†å¹³å°è”åæƒç›ŠåŒ…ã€‚
        """,
        "é’ˆå¯¹Zæ—¶ä»£ç”¨æˆ·çš„è¿è¥å»ºè®®": f"""
è¿è¥å»ºè®®ï¼ˆè¦†ç›–{total_rows}åZä¸–ä»£çœŸå®ç”¨æˆ·ï¼‰ï¼š
1. äº§å“ï¼šæµé‡+ç½‘æ¸¸/çŸ­è§†é¢‘ä¼šå‘˜èåˆå¥—é¤ï¼ˆåŒ¹é…ç”¨æˆ·æ ¸å¿ƒAPPä½¿ç”¨ä¹ æƒ¯ï¼‰ï¼›
2. æ¸ é“ï¼šé«˜æ ¡/å•†åœˆåœ°æ¨ï¼ˆCSVä¸­{int([col for col in df.columns if 'SCHOOL' in col.upper()] and df[col].mean() * 100)}%ç”¨æˆ·ä¸ºæ ¡å›­ç¾¤ä½“ï¼‰ï¼Œå¹´è½»åŒ–è¥é”€å†…å®¹ï¼›
3. æƒç›Šï¼šè”åˆæ–‡æ—…/ç”µç«èµ›äº‹ï¼Œæ¨å‡ºä¸“å±æµé‡åŒ…ï¼›
4. æœåŠ¡ï¼š95åä¸“å±å®¢æœé€šé“ï¼Œæå‡å“åº”æ•ˆç‡ã€‚
        """
    }
    return jsonify({
        "status": "success",
        "answer": ai_answers.get(query.strip(), default_answer)
    })


# ========== å¯åŠ¨æœåŠ¡ ==========
if __name__ == '__main__':
    app.run(debug=True)